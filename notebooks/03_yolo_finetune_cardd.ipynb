{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","toc_visible":true,"mount_file_id":"1fXzoF5DH_g4O5c0uopMsMmqzFGrlqC9K","authorship_tag":"ABX9TyNxu/GZYxBrj73gnYTtj5Jy"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12681317,"sourceType":"datasetVersion","datasetId":8013844}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gabrielfcarvalho/yolo-finetune-cardd?scriptVersionId=254364103\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# YOLO11 Fine-Tuning for Car Damage on CarDD Dataset","metadata":{"id":"yC0z2NqXtBYa"}},{"cell_type":"markdown","source":"# üìå **Notebook Overview**\n# This notebook demonstrates fine-tuning the YOLO11 for detecting car damages.\n# It follows these steps:\n# 1Ô∏è‚É£ **Install and import the required libraries.**\n# 2Ô∏è‚É£ **Convert the CarDD dataset from COCO format to YOLO format. (already did in this dataset)**\n# 3Ô∏è‚É£ **Create the data.yaml configuration file for YOLO. (already did in this dataset)**\n# 4Ô∏è‚É£ **Train a YOLO model on the converted dataset.**\n# 5Ô∏è‚É£ **Validate the trained model (mAP, precision, recall, etc.).**\n# 6Ô∏è‚É£ **Perform inference on test images and compare Ground Truth vs. Predictions.**\n# 7Ô∏è‚É£ **Perform inference on real car images**","metadata":{"id":"qrm_f9gZtO_p"}},{"cell_type":"markdown","source":"# üìå 1Ô∏è‚É£ Install Required Libraries","metadata":{"id":"KZQPszh4tnY1"}},{"cell_type":"code","source":"!pip install ultralytics\n!pip install pycocotools opencv-python tqdm\n","metadata":{"id":"kUSoSjvIqHY3","executionInfo":{"status":"ok","timestamp":1738782200169,"user_tz":180,"elapsed":68242,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"3cfe2dbc-0130-476b-fa0f-981492c6c2c3","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T14:24:42.916785Z","iopub.execute_input":"2025-08-05T14:24:42.917078Z","iopub.status.idle":"2025-08-05T14:26:58.173323Z","shell.execute_reply.started":"2025-08-05T14:24:42.917055Z","shell.execute_reply":"2025-08-05T14:26:58.172521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport shutil\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n","metadata":{"id":"8Dg1RedJqoLd","executionInfo":{"status":"ok","timestamp":1738782200206,"user_tz":180,"elapsed":26,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T14:27:07.231689Z","iopub.execute_input":"2025-08-05T14:27:07.231948Z","iopub.status.idle":"2025-08-05T14:27:07.409204Z","shell.execute_reply.started":"2025-08-05T14:27:07.231926Z","shell.execute_reply":"2025-08-05T14:27:07.408703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 2Ô∏è‚É£ Convert COCO to YOLO Format (This was used before to make the dataset based on the previously availabe COCO dataset in https://cardd-ustc.github.io/)","metadata":{"id":"WOcrbeonuTof"}},{"cell_type":"code","source":"# def convert_coco_to_yolo(annotations_file, images_folder, output_folder, categories):\n#     \"\"\"\n#     Reads a COCO file (annotations_file) and converts it to YOLO format,\n#     copying images to output_folder/images and labels to output_folder/labels.\n\n#     - annotations_file: path to a COCO .json (e.g. 'instances_train2017.json')\n#     - images_folder: folder with the corresponding images (e.g. 'train2017/')\n#     - output_folder: base output folder (e.g. 'train/')\n#     - categories: dictionary {cat_id: yolo_idx}, mapping COCO category_id -> YOLO class index\n#     \"\"\"\n#     images_output = os.path.join(output_folder, 'images')\n#     labels_output = os.path.join(output_folder, 'labels')\n#     os.makedirs(images_output, exist_ok=True)\n#     os.makedirs(labels_output, exist_ok=True)\n\n#     with open(annotations_file, 'r') as f:\n#         coco_data = json.load(f)\n\n#     # Store image info by \"image_id\"\n#     images_dict = {}\n#     for img_info in coco_data['images']:\n#         images_dict[img_info['id']] = {\n#             'file_name': img_info['file_name'],\n#             'width': img_info['width'],\n#             'height': img_info['height']\n#         }\n\n#     # Prepare dictionary to hold YOLO lines for each image\n#     yolo_annotations = {img_id: [] for img_id in images_dict}\n\n#     # Iterate over all COCO annotations\n#     for ann in tqdm(coco_data['annotations'], desc=f\"Converting {os.path.basename(annotations_file)}\"):\n#         img_id = ann['image_id']\n\n#         cat_id = ann['category_id']\n#         if cat_id not in categories:\n#             # Optionally raise a warning\n#             continue\n#         yolo_class = categories[cat_id]\n\n#         # COCO bbox: [x_min, y_min, width, height]\n#         x_min, y_min, w, h = ann['bbox']\n#         img_w = images_dict[img_id]['width']\n#         img_h = images_dict[img_id]['height']\n\n#         # Convert to YOLO [x_center, y_center, w, h] normalized\n#         x_center = (x_min + w / 2) / img_w\n#         y_center = (y_min + h / 2) / img_h\n#         w_norm = w / img_w\n#         h_norm = h / img_h\n\n#         yolo_line = f\"{yolo_class} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\"\n#         yolo_annotations[img_id].append(yolo_line)\n\n#     # Copy images and write label files\n#     for img_id, lines in yolo_annotations.items():\n#         file_name = images_dict[img_id]['file_name']\n#         src_img_path = os.path.join(images_folder, file_name)\n#         dst_img_path = os.path.join(images_output, file_name)\n\n#         # Copy the image\n#         if os.path.exists(src_img_path):\n#             shutil.copy(src_img_path, dst_img_path)\n#         else:\n#             print(f\"Warning: image {src_img_path} not found!\")\n\n#         # Create the .txt file\n#         base_name = os.path.splitext(file_name)[0]\n#         txt_path = os.path.join(labels_output, base_name + '.txt')\n\n#         with open(txt_path, 'w') as f_txt:\n#             for line in lines:\n#                 f_txt.write(line + '\\n')\n","metadata":{"id":"mk_KPtacqpp2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Map COCO category_id -> YOLO class index\n# categories_dict = {\n#     1: 0,  # dent\n#     2: 1,  # scratch\n#     3: 2,  # crack\n#     4: 3,  # glass shatter\n#     5: 4,  # lamp broken\n#     6: 5   # tire flat\n# }\n","metadata":{"id":"NuvUPoEpquAc"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Caminhos\n# BASE_DIR = 'CarDD_COCO'\n# ann_dir = os.path.join(BASE_DIR, 'annotations')\n\n# train_json = os.path.join(ann_dir, 'instances_train2017.json')\n# val_json   = os.path.join(ann_dir, 'instances_val2017.json')\n# test_json  = os.path.join(ann_dir, 'instances_test2017.json')\n\n# train_imgs = os.path.join(BASE_DIR, 'train2017')\n# val_imgs   = os.path.join(BASE_DIR, 'val2017')\n# test_imgs  = os.path.join(BASE_DIR, 'test2017')\n\n# # Pastas de sa√≠da YOLO\n# OUTPUT_DIR = os.path.join('CarDD_release', 'CarDD_YOLO')\n# train_out = os.path.join(OUTPUT_DIR, 'train')\n# val_out   = os.path.join(OUTPUT_DIR, 'val')\n# test_out  = os.path.join(OUTPUT_DIR, 'test')\n\n# os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# categories_dict = {\n#     1: 0,  # dent\n#     2: 1,  # scratch\n#     3: 2,  # crack\n#     4: 3,  # glass shatter\n#     5: 4,  # lamp broken\n#     6: 5   # tire flat\n# }\n\n# # Converter\n# convert_coco_to_yolo(train_json, train_imgs, train_out, categories_dict)\n# convert_coco_to_yolo(val_json,   val_imgs,   val_out,   categories_dict)\n# convert_coco_to_yolo(test_json,  test_imgs,  test_out,  categories_dict)\n","metadata":{"id":"lTL7zu6fqzDq","executionInfo":{"status":"ok","timestamp":1738703609660,"user_tz":180,"elapsed":2313741,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"67c5f77a-18be-448b-e085-235af0dfb323"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 3Ô∏è‚É£ Making the data.yaml (already available on the dataset)","metadata":{"id":"oOUiT6iDuv6_"}},{"cell_type":"code","source":"# data_yaml = f\"\"\"\n# # COCO-like Car Damage Dataset converted to YOLO\n# train: {os.path.join(OUTPUT_DIR, 'train/images')}\n# val: {os.path.join(OUTPUT_DIR, 'val/images')}\n# test: {os.path.join(OUTPUT_DIR, 'test/images')}  # opcional, se quiser avaliar\n\n# nc: 6\n# names: ['dent', 'scratch', 'crack', 'glass shatter', 'lamp broken', 'tire flat']\n# \"\"\"\n\n# with open(os.path.join(OUTPUT_DIR, 'data.yaml'), 'w') as f:\n#     f.write(data_yaml)\n\n# print(data_yaml)\n","metadata":{"id":"FjbXO13j1inl","executionInfo":{"status":"ok","timestamp":1738704081013,"user_tz":180,"elapsed":379,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"5075da86-1c12-40b5-f2e0-8c801a0d150b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 4Ô∏è‚É£ Train the YOLO Model","metadata":{"id":"ieOp4XJ9u4x6"}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('yolo11n.pt')\n\n\nresults = model.train(\n    data='/kaggle/input/cardd-with-yolo-annotations-images-labels/data.yaml',\n    epochs=50,\n    batch=16,\n    imgsz=640,\n    name='yolov11_carDD_exp1'\n)\n","metadata":{"id":"_7kMF5kp11RR","executionInfo":{"status":"ok","timestamp":1738707282409,"user_tz":180,"elapsed":2106794,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"e3cf36fe-3a4b-4c18-e2a2-2013e2db5f69","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T14:27:18.516097Z","iopub.execute_input":"2025-08-05T14:27:18.516377Z","iopub.status.idle":"2025-08-05T15:06:06.097809Z","shell.execute_reply.started":"2025-08-05T14:27:18.516355Z","shell.execute_reply":"2025-08-05T15:06:06.096853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 5Ô∏è‚É£ Validate the YOLO Model (Val and Test splits)","metadata":{"id":"elaSZp78vAMb"}},{"cell_type":"code","source":"model = YOLO('/kaggle/working/runs/detect/yolov11_carDD_exp1/weights/best.pt')\n\nmetrics = model.val(data='/kaggle/input/cardd-with-yolo-annotations-images-labels/data.yaml', imgsz=640, batch=16, conf=0.25, iou=0.6, device=\"0\")\n","metadata":{"id":"WhhmL1YvlanD","executionInfo":{"status":"ok","timestamp":1738716798385,"user_tz":180,"elapsed":90743,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"63a0f8e9-989f-4f48-9ed7-de691fb8d116","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T15:06:55.021442Z","iopub.execute_input":"2025-08-05T15:06:55.022273Z","iopub.status.idle":"2025-08-05T15:07:07.483707Z","shell.execute_reply.started":"2025-08-05T15:06:55.02224Z","shell.execute_reply":"2025-08-05T15:07:07.483015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Overall mAP50-95: {metrics.box.map:.4f}\")\nprint(f\"Overall mAP50:   {metrics.box.map50:.4f}\")\nprint(f\"Overall mAP75:   {metrics.box.map75:.4f}\")\n\n# Assuming 'metrics.names' contains category names\nprint(\"\\nmAP50-95 per category:\")\nfor i, map_val in enumerate(metrics.box.maps):\n    print(f\"- {metrics.names[i]}: {map_val:.4f}\")","metadata":{"id":"gScOR6ULmK29","executionInfo":{"status":"ok","timestamp":1738716911774,"user_tz":180,"elapsed":1029,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"44dec4cc-ad37-4351-db47-87db8890437f","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T15:07:11.396313Z","iopub.execute_input":"2025-08-05T15:07:11.396638Z","iopub.status.idle":"2025-08-05T15:07:11.402458Z","shell.execute_reply.started":"2025-08-05T15:07:11.396613Z","shell.execute_reply":"2025-08-05T15:07:11.40185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO('/kaggle/working/runs/detect/yolov11_carDD_exp1/weights/best.pt')\n\nmetrics = model.val(data='/kaggle/input/cardd-with-yolo-annotations-images-labels/data.yaml', imgsz=640, batch=16, conf=0.25, iou=0.6, device=\"0\", split = 'test')","metadata":{"id":"Du4sgNtFn5Wm","executionInfo":{"status":"ok","timestamp":1738717392284,"user_tz":180,"elapsed":105491,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"03b471e4-1cbc-4011-8ed2-2fb2b1febaf6","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T15:07:14.228963Z","iopub.execute_input":"2025-08-05T15:07:14.229722Z","iopub.status.idle":"2025-08-05T15:07:25.282398Z","shell.execute_reply.started":"2025-08-05T15:07:14.229687Z","shell.execute_reply":"2025-08-05T15:07:25.28177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Overall mAP50-95: {metrics.box.map:.4f}\")\nprint(f\"Overall mAP50:   {metrics.box.map50:.4f}\")\nprint(f\"Overall mAP75:   {metrics.box.map75:.4f}\")\n\n# Assuming 'metrics.names' contains category names\nprint(\"\\nmAP50-95 per category:\")\nfor i, map_val in enumerate(metrics.box.maps):\n    print(f\"- {metrics.names[i]}: {map_val:.4f}\")","metadata":{"id":"Q9tv3ZW0oGyc","executionInfo":{"status":"ok","timestamp":1738717433591,"user_tz":180,"elapsed":1004,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"17d28313-3c86-4586-df69-244f7eb5aa11","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T15:07:28.225957Z","iopub.execute_input":"2025-08-05T15:07:28.226285Z","iopub.status.idle":"2025-08-05T15:07:28.231936Z","shell.execute_reply.started":"2025-08-05T15:07:28.22626Z","shell.execute_reply":"2025-08-05T15:07:28.231299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 6Ô∏è‚É£ Test YOLO Model Inference and Visualization","metadata":{"id":"ibf-kwgHvTtM"}},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\n\n############################################\n# 1) Configurations\n############################################\n\nmodel_path = '/kaggle/working/runs/detect/yolov11_carDD_exp1/weights/best.pt'\nmodel = YOLO(model_path)\n\nTEST_IMAGES_DIR = '/kaggle/input/cardd-with-yolo-annotations-images-labels/test/images'\nTEST_LABELS_DIR = '/kaggle/input/cardd-with-yolo-annotations-images-labels/test/labels'\n\nCLASS_NAMES = {\n    0: 'dent',\n    1: 'scratch',\n    2: 'crack',\n    3: 'glass shatter',\n    4: 'lamp broken',\n    5: 'tire flat'\n}\n\n############################################\n# 2) Utility Functions\n############################################\n\ndef yolo_to_xyxy(yolo_box, img_w, img_h):\n    \"\"\"\n    Convert YOLO box [cls, x_center, y_center, w, h] (normalized)\n    to [x1, y1, x2, y2] in pixel coords.\n    \"\"\"\n    cls, cx, cy, w, h = yolo_box\n    x1 = (cx - w/2) * img_w\n    y1 = (cy - h/2) * img_h\n    x2 = (cx + w/2) * img_w\n    y2 = (cy + h/2) * img_h\n    return [x1, y1, x2, y2]\n\ndef read_ground_truth_txt(txt_path, img_w, img_h):\n    \"\"\"\n    Reads a YOLO-format .txt and returns boxes as [x1, y1, x2, y2, cls].\n    \"\"\"\n    bboxes = []\n    if not os.path.exists(txt_path):\n        return bboxes\n    with open(txt_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) != 5:\n                continue\n            cls_id = int(parts[0])\n            x_center = float(parts[1])\n            y_center = float(parts[2])\n            w_norm = float(parts[3])\n            h_norm = float(parts[4])\n            x1, y1, x2, y2 = yolo_to_xyxy((cls_id, x_center, y_center, w_norm, h_norm), img_w, img_h)\n            bboxes.append([x1, y1, x2, y2, cls_id])\n    return bboxes\n\ndef draw_bboxes(image, bboxes, color, is_pred=False):\n    \"\"\"\n    Draw bounding boxes on the image.\n    If is_pred=True, bboxes have 6 items (including confidence).\n    \"\"\"\n    out_img = image.copy()\n    for box in bboxes:\n        if is_pred:\n            x1, y1, x2, y2, cls_id, conf = box\n            label = f\"{CLASS_NAMES.get(cls_id, str(cls_id))} ({conf:.2f})\"\n        else:\n            x1, y1, x2, y2, cls_id = box\n            label = f\"{CLASS_NAMES.get(cls_id, str(cls_id))}\"\n        cv2.rectangle(out_img, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n        cv2.putText(\n            out_img, label, (int(x1), max(0, int(y1) - 5)),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2\n        )\n    return out_img\n\n############################################\n# 3) Run Inference on Sample Test Images\n############################################\n\nall_test_images = [f for f in os.listdir(TEST_IMAGES_DIR) if f.lower().endswith('.jpg')]\n\n# For instance, pick 6 random images\nsample_imgs = random.sample(all_test_images, k=6)\n\nfor img_name in sample_imgs:\n    img_path = os.path.join(TEST_IMAGES_DIR, img_name)\n    img_bgr = cv2.imread(img_path)\n    if img_bgr is None:\n        print(f\"Could not read {img_name}\")\n        continue\n\n    img_h, img_w = img_bgr.shape[:2]\n\n    # Ground Truth\n    txt_path = os.path.join(TEST_LABELS_DIR, os.path.splitext(img_name)[0] + '.txt')\n    gt_bboxes = read_ground_truth_txt(txt_path, img_w, img_h)\n\n    # Prediction\n    results = model.predict(source=img_path, conf=0.25)\n    pred_bboxes = []\n    for box in results[0].boxes:\n        x1y1x2y2 = box.xyxy[0].cpu().numpy().tolist()\n        conf = float(box.conf[0].cpu().numpy())\n        cls_id = int(box.cls[0].cpu().numpy())\n        pred_bboxes.append([x1y1x2y2[0], x1y1x2y2[1],\n                            x1y1x2y2[2], x1y1x2y2[3],\n                            cls_id, conf])\n\n    # Draw images separately\n    gt_img = draw_bboxes(img_bgr, gt_bboxes, color=(0,255,0), is_pred=False)\n    pred_img = draw_bboxes(img_bgr, pred_bboxes, color=(0,0,255), is_pred=True)\n\n    # Convert BGR->RGB for Matplotlib\n    gt_img_rgb = cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB)\n    pred_img_rgb = cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB)\n\n    # Plot side by side\n    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n    axes[0].imshow(gt_img_rgb)\n    axes[0].set_title(f\"Ground Truth - {img_name}\")\n    axes[0].axis('off')\n\n    axes[1].imshow(pred_img_rgb)\n    axes[1].set_title(f\"Prediction - {img_name}\")\n    axes[1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"id":"wb3rSAQtoO9p","executionInfo":{"status":"ok","timestamp":1738717521257,"user_tz":180,"elapsed":23554,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"c5dffcf8-9fa6-4c23-e174-45a198ddc05f","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T15:07:32.219115Z","iopub.execute_input":"2025-08-05T15:07:32.2198Z","iopub.status.idle":"2025-08-05T15:07:36.307994Z","shell.execute_reply.started":"2025-08-05T15:07:32.219776Z","shell.execute_reply":"2025-08-05T15:07:36.307151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå 7Ô∏è‚É£ **Perform inference on real car images (to be done after adding new images to the input)**","metadata":{"id":"7Sq18Y1rfBg-"}},{"cell_type":"code","source":"# import os\n# import random\n# import cv2\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from ultralytics import YOLO\n\n# ############################################\n# # 1) Configurations\n# ############################################\n\n# model_path = '/kaggle/working/runs/detect/yolov11_carDD_exp1/weights/best.pt'\n# model = YOLO(model_path)\n\n# CAR_IMAGES_DIR = 'Carro_Professor'\n\n# CLASS_NAMES = {\n#     0: 'dent',\n#     1: 'scratch',\n#     2: 'crack',\n#     3: 'glass shatter',\n#     4: 'lamp broken',\n#     5: 'tire flat'\n# }\n\n# ############################################\n# # 2) Utility Functions\n# ############################################\n\n# def draw_bboxes(image, bboxes, color):\n#     \"\"\"\n#     Draw bounding boxes on the image.\n#     \"\"\"\n#     out_img = image.copy()\n#     for box in bboxes:\n#         x1, y1, x2, y2, cls_id, conf = box\n#         label = f\"{CLASS_NAMES.get(cls_id, str(cls_id))} ({conf:.2f})\"\n#         cv2.rectangle(out_img, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n#         cv2.putText(\n#             out_img, label, (int(x1), max(0, int(y1) - 5)),\n#             cv2.FONT_HERSHEY_SIMPLEX, 2, color, 3\n#         )\n#     return out_img\n\n# ############################################\n# # 3) Run Inference on Sample Test Images\n# ############################################\n\n# all_test_images = [f for f in os.listdir(CAR_IMAGES_DIR) if f.lower().endswith('.jpg')]\n\n# # For instance, pick 6 random images\n# sample_imgs = random.sample(all_test_images, k=6)\n\n# for img_name in sample_imgs:\n#     img_path = os.path.join(CAR_IMAGES_DIR, img_name)\n#     img_bgr = cv2.imread(img_path)\n#     if img_bgr is None:\n#         print(f\"Could not read {img_name}\")\n#         continue\n\n#     # Run prediction\n#     results = model.predict(source=img_path, conf=0.25)\n#     pred_bboxes = []\n#     for box in results[0].boxes:\n#         x1y1x2y2 = box.xyxy[0].cpu().numpy().tolist()\n#         conf = float(box.conf[0].cpu().numpy())\n#         cls_id = int(box.cls[0].cpu().numpy())\n#         pred_bboxes.append([x1y1x2y2[0], x1y1x2y2[1],\n#                             x1y1x2y2[2], x1y1x2y2[3],\n#                             cls_id, conf])\n\n#     # Draw predictions on the image\n#     pred_img = draw_bboxes(img_bgr, pred_bboxes, color=(0,0,255))\n\n#     # Convert BGR->RGB for Matplotlib\n#     pred_img_rgb = cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB)\n\n#     # Show only the predicted image\n#     plt.figure(figsize=(15, 15))\n#     plt.imshow(pred_img_rgb)\n#     plt.title(f\"Predictions - {img_name}\")\n#     plt.axis('off')\n#     plt.show()\n","metadata":{"id":"b7d5JKJyfEKY","executionInfo":{"status":"ok","timestamp":1738782637960,"user_tz":180,"elapsed":10200,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"6e3b8e2c-0580-437c-860f-bf6dbd0151f9"},"outputs":[],"execution_count":null}]}